{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "praveenkumardev"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/dataflow1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Json4",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable3",
								"type": "DatasetReference"
							},
							"name": "sink1",
							"rejectedDataLinkedService": {
								"referenceName": "lnk_adls",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "Flattern1"
						},
						{
							"name": "flatten1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          audit as (createdBy as string, createdOn as timestamp, modifiedOn as timestamp),",
						"          contact as (address as (city as string, line1 as string, line2 as string, pincode as integer), email as string, phones as (number as string, type as string)[]),",
						"          employeeId as string,",
						"          name as (first as string, last as string),",
						"          projects as (name as string, projectId as string, role as string, tasks as (description as string, status as string, taskId as string)[], technologies as string[])[],",
						"          skills as string[]",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'arrayOfDocuments',",
						"     partitionBy('hash', 1)) ~> source1",
						"source1 foldDown(unroll(projects, projects),",
						"     mapColumn(",
						"          name = projects.name,",
						"          projectId = projects.projectId,",
						"          role = projects.role,",
						"          tasks = projects.tasks,",
						"          technologies = projects.technologies",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> Flattern1",
						"Flattern1 foldDown(unroll(tasks, tasks),",
						"     mapColumn(",
						"          description = tasks.description,",
						"          status = tasks.status,",
						"          taskId = tasks.taskId",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EmployeeId as string,",
						"          FirstName as string,",
						"          LastName as string,",
						"          Email as string,",
						"          AddressLine1 as string,",
						"          AddressLine2 as string,",
						"          City as string,",
						"          Pincode as string,",
						"          CreatedOn as timestamp,",
						"          ModifiedOn as timestamp,",
						"          CreatedBy as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          EmployeeId = description,",
						"          FirstName = status,",
						"          LastName = taskId",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Json6",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable5",
								"type": "DatasetReference"
							},
							"name": "sink1",
							"rejectedDataLinkedService": {
								"referenceName": "lnk_adls",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employeeId as string,",
						"          name as (first as string, last as string),",
						"          contact as (email as string, phones as (type as string, number as string)[], address as (line1 as string, line2 as string, city as string, pincode as string)),",
						"          skills as string[],",
						"          projects as (projectId as string, name as string, role as string, technologies as string[], tasks as (taskId as string, description as string, status as string)[])[],",
						"          audit as (createdOn as string, modifiedOn as string, createdBy as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'arrayOfDocuments',",
						"     partitionBy('hash', 1)) ~> source1",
						"source1 foldDown(unrollMultiple(contact.phones,skills,projects,projects.technologies,projects.tasks),",
						"     mapColumn(",
						"          employeeId,",
						"          {name.first} = name.first,",
						"          {name.last} = name.last,",
						"          email = contact.email,",
						"          {address.line1} = contact.address.line1,",
						"          {address.line2} = contact.address.line2,",
						"          city = contact.address.city,",
						"          pincode = contact.address.pincode,",
						"          createdOn = audit.createdOn,",
						"          modifiedOn = audit.modifiedOn,",
						"          createdBy = audit.createdBy",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> flatten1",
						"flatten1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EmployeeId as string,",
						"          FirstName as string,",
						"          LastName as string,",
						"          Email as string,",
						"          AddressLine1 as string,",
						"          AddressLine2 as string,",
						"          City as string,",
						"          Pincode as string,",
						"          CreatedOn as timestamp,",
						"          ModifiedOn as timestamp,",
						"          CreatedBy as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          EmployeeId = employeeId,",
						"          FirstName = {name.first},",
						"          LastName = {name.last},",
						"          Email = email,",
						"          AddressLine1 = {address.line1},",
						"          AddressLine2 = {address.line2},",
						"          City = city,",
						"          Pincode = pincode,",
						"          CreatedOn = createdOn,",
						"          ModifiedOn = modifiedOn,",
						"          CreatedBy = createdBy",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow3')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DelimitedText48",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable28",
								"type": "DatasetReference"
							},
							"name": "sink1",
							"rejectedDataLinkedService": {
								"referenceName": "lnk_adls",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as double",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 alterRow(upsertIf(empid==empid)) ~> alterRow1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: true,",
						"     input(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2)",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:false,",
						"     upsertable:true,",
						"     keys:['empid'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          empid,",
						"          empname,",
						"          empdept,",
						"          empqual,",
						"          empjoindate,",
						"          empsal",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow5')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DelimitedText50",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTable29",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable30",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTable31",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "exists1"
						},
						{
							"name": "exists2"
						},
						{
							"name": "AlterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as string,",
						"          empsal as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2),",
						"          active_status as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source2",
						"source1 derive(hashkey1 = md5(empsal,empdept),",
						"          status1 = 1) ~> derivedColumn1",
						"source2 derive(hashkey2 = md5(empdept,empsal),",
						"          status2 = 2) ~> derivedColumn2",
						"derivedColumn1, derivedColumn2 exists(hashkey1==hashkey2,",
						"     negate:true,",
						"     broadcast: 'auto')~> exists1",
						"derivedColumn2, exists1 exists(source2@empname==source1@empname&&hashkey1!=hashkey2,",
						"     negate:true,",
						"     broadcast: 'auto')~> exists2",
						"exists2 alterRow(updateIf(1==1)) ~> AlterRow1",
						"exists1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2),",
						"          active_status as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          empid,",
						"          empname,",
						"          empdept,",
						"          empqual,",
						"          empjoindate,",
						"          empsal,",
						"          active_status = status1",
						"     )) ~> sink1",
						"AlterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2),",
						"          active_status as string",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['empid'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          empid,",
						"          empname,",
						"          empdept,",
						"          empqual,",
						"          empjoindate,",
						"          empsal,",
						"          active_status",
						"     )) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow6')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DelimitedText53",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTable32",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable33",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTable34",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "exists1"
						},
						{
							"name": "exists2"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,0)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2),",
						"          active_status as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source2",
						"source1 derive(hashkey1 = md5(empdept,empsal),",
						"          status1 = 1) ~> derivedColumn1",
						"source2 derive(hashkey2 = md5(empdept,empsal),",
						"          status0 = 0) ~> derivedColumn2",
						"derivedColumn1, derivedColumn2 exists(source1@empname==source2@empname&&hashkey1==hashkey2,",
						"     negate:true,",
						"     broadcast: 'auto')~> exists1",
						"derivedColumn2, exists1 exists(hashkey1==hashkey2,",
						"     negate:true,",
						"     broadcast: 'auto')~> exists2",
						"exists2 alterRow(upsertIf(1==1)) ~> alterRow1",
						"exists1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2),",
						"          active_status as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          empid,",
						"          empname,",
						"          empdept,",
						"          empqual,",
						"          empjoindate,",
						"          empsal,",
						"          active_status = status1",
						"     )) ~> sink1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2),",
						"          active_status as string",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['empid'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          empid,",
						"          empname,",
						"          empdept,",
						"          empqual,",
						"          empjoindate,",
						"          empsal,",
						"          active_status",
						"     )) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow7')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DelimitedText56",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTable35",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable36",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTable37",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "exists1"
						},
						{
							"name": "exists2"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,0)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2),",
						"          active_status as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source2",
						"source1 derive(hashkey1 = md5(empdept,empsal),",
						"          status1 = 1==1) ~> derivedColumn1",
						"source2 derive(hashkey2 = md5(empdept,empsal),",
						"          status0 = 0) ~> derivedColumn2",
						"derivedColumn1, derivedColumn2 exists(source1@empid==source2@empid&& hashkey1==hashkey2,",
						"     negate:true,",
						"     broadcast: 'auto')~> exists1",
						"derivedColumn2, exists1 exists(hashkey1==hashkey2,",
						"     negate:true,",
						"     broadcast: 'auto')~> exists2",
						"exists2 alterRow(upsertIf(1==1)) ~> alterRow1",
						"exists1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2),",
						"          active_status as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          empid,",
						"          empname,",
						"          empdept,",
						"          empqual,",
						"          empjoindate,",
						"          empsal,",
						"          active_status = status1",
						"     )) ~> sink1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          empname as string,",
						"          empdept as string,",
						"          empqual as string,",
						"          empjoindate as date,",
						"          empsal as decimal(10,2),",
						"          active_status as string",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['empid'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/pipeline2')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflow1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "praveendev"
				},
				"annotations": [],
				"lastPublishTime": "2025-11-24T06:37:34Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dataflow1')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pl_njson_sql')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "df_njson",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflow2",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "praveendev"
				},
				"annotations": [],
				"lastPublishTime": "2025-11-24T09:48:41Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dataflow2')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/scd2')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflow6",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"source2": {},
									"sink1": {},
									"sink2": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "reddy"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dataflow6')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/scd2_0')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflow7",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"source2": {},
									"sink1": {},
									"sink2": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "reddy"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dataflow7')]"
			]
		}
	]
}